---
header-includes: \usepackage{amssymb} \usepackage{hyperref} \usepackage{booktabs} \hypersetup{ colorlinks=true,
  linkcolor=blue, filecolor=magenta, urlcolor=cyan}
bibliography: ../biblio.bib
bibliographystyle: apalike
nocite: | 
  @*
output:
  pdf_document: default
---
\newpage
\begin{flushright}
    \textbf{Équipe 8}
\end{flushright}

\begin{center}
    \vspace{2\baselineskip}
    Charles Comeau \\
    (111 185 421) \\
    \vspace{1\baselineskip}
    Nicholas Langevin \\
    (111 184 631) \\
    \vspace{1\baselineskip}
    Andréanne Larouche \\
    (111 190 518) \\
    \vspace{7\baselineskip}
    Apprentissage statistique en actuariat\\
    ACT-3114 \\
    \vspace{7\baselineskip}
    {\large
    \textbf{Analyse des données de renouvellement d'assurance}} \\
    \vspace{8\baselineskip}
    présenté à \\
    Marie-Pier Côté \\
    \vspace{9\baselineskip}
    École d’actuariat \\
    Université Laval \\
    27 février 2020
\end{center}


\newpage

\tableofcontents

```{r, setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 5, echo=FALSE, message=FALSE, warning=FALSE,
fig.align="center")
```

```{r, LoadingPackages, include = FALSE}
library(plyr)
library(tidyverse)
library(pROC)
library(rpart)
library(rpart.plot)
source("../script/_utilityFunction.R")
load(file="../data/trainData.RData")
load(file="../data/testData.RData")
```

# Ajustement des modèles

## k plus proches voisins

```{r, knn.bestTune}
load(file="../src/03-knn/knnTrain(fit.knn).rds")
load(file="../src/03-knn/knnTrainWithoutSmote(fit.knn).rds")
```

Le modèle des _k_ plus proches voisins calcule les prédictions en utilisant la
distance euclédienne, ce qui veut dire que les variables catégorielles
non-ordonées doivent être transformées en $(c-1)$ variables binaires. Aussi,
les variables catégorielles ordonnées ont été transformées en des variables
numériques disctrètes $(1, 2, 3,...)$. Le modèle des _k_ plus proches voisins
comprend un seul hyperparamètre, soit le nombre de voisins (_k_) à utilité pour
la prédiction. Ce choix a été optimizer en utilisant une validation croisé à 4
plis. La valeur de _k_ qui maximise l'aire sour la courbe ROC (_fonction
d’efficacité du récepteur_) est de `r as.numeric(fit.knn$bestTune)`. Ces
valeurs, pour chaque valeurs de _k_, sont présenté dans la
\autoref{tbl:knn.plot-bestTune}.


```{r, knn.plot_bestTune, results='asis', fig.asp=.62, fig.cap="\\label{tbl:knn.plot-bestTune} Valeurs de l'aire sous la courbe ROC en fonction de différentes valeurs de \\emph{k}"}
plot(fit.knn, xlab = "k", ylab = "AUC (Validation croisée)")
```

```{r, knn.plot_ROC, results='asis', fig.height=4, fig.cap="\\label{tbl:knn.plot-ROC} TODO"}
load(file="../src/03-knn/knnModele(modele.knn).rds")
# ROC(dataToNumeric(testData, "y"), modele.knn$pred, col="blue")
# roc(dataToNumeric(testData, "y"), modele.knn$pred, plot=TRUE)
```
\newpage

## Arbre de classification

```{r, tree.bestTune}
load(file="../src/04-tree/tree.T0.rds")
load(file="../src/04-tree/tree.gridResult.info.rds")
load(file="../src/04-tree/tree.gridResult.gini.rds")
load(file="../src/04-tree/tree.final.rds")
# fonction de perte
# procedure choix final
# presenter les hyperparamètre
```

Le modèle d'arbre de classification offre la possibilité d'utilisé deux
fonctions de perte différence. Si _K_ représente le nombre de classe, alors un
premier choix comme fonction de perte peut être l'indice de _Gini_, defini
comme
\begin{align}
    \mathcal{L}_G = \sum_{k=1}^K \widehat{P}_{mk} (1 - \widehat{P}_{mk}).
\end{align}
Dans un deuxième cas, il est possible d'utilisé la _deviance_ négative d'une
bernouilli (ou encore l'_entropie croisée_) defini comme
\begin{align}
    \mathcal{L}_D = \sum_{k=1}^K \widehat{P}_{mk} \ln( \widehat{P}_{mk}).
\end{align} 
La procédure d'optimisation des hyperparamètre à été effectuer pour chacune des
deux fonction de perte. En premier lieux, un arbre complet $T_0$ à été
entrainer par validation croisée en 10 plis. Celui-ci est défini comme un arbre
construit avec un paramètre de complexité (_cp_) à 0 ainsi que le nombre 
minimal d'observation dans chaque feuille (_minbucket_) à 1. Le
\autoref{fig:tree.plotArbreT0} présente l'erreur obtenu par validation croisée
pour certaine valeurs de _cp_. Selon cette metrique, le meilleur modèle serait 
l'arbre racine, ce qui signifie que la prédiction serait la même pour toutes
les données. La metrique _AUC_ sera considéré par la suite pour l'optimisation
des hyperparamètres.
```{r, tree.plotArbreT0, results='asis', fig.height=4, fig.cap="Résultats de la validation croisée pour un arbre complet $T_0$ utilisant \\emph{Gini} comme fonction de perte."}
plotcp(tree.T0, lty=2, col=2)
```

\newpage
Pour la suite, l'optimisation du _cp_ à été efectué sur plusieurs valeurs de
_minbucket_. La \autoref{tbl:tree.auc} montre les résultat de de l'aire sous la courbe ROC
(_AUC_) pour différente valeur de _minbucket_ en fonction du _cp_ optimal. À
droite, la fonction de perte utilisée est l'indice de _Gini_, alors qu'à
gauche, la fonction de perte utilité est l'_entropie croisée_. Ces valeurs on
été obtenu par validation croisée à 4 plis.


\begin{table}[!ht]
\caption{\emph{AUC} pour différente valeur de \emph{minbucket} en fonction du 
\emph{cp} optimal. À gauche, la fonction de perte utilisée est l'indice de 
\emph{Gini}, à droite l'\emph{entropie croisée}.}
\label{tbl:tree.auc}
\begin{minipage}{0.48\linewidth}
\centering
\begin{tabular}{ccc}
\hline
\multicolumn{3}{c}{Indice de Gini} \\
```{r, results='asis'}
Rmd_table(tree.gridResult.gini, caption="", label="",
          digits=c(0,0,6, 3), hline.after=c(-1, 0, 6, 7, 11), 
          only.contents = TRUE)
```
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
\centering
\begin{tabular}{ccc}
\hline
\multicolumn{3}{c}{Entropie croisée} \\
```{r, results='asis'}
Rmd_table(tree.gridResult.info, caption="", label="",
          digits=c(0,0,6,3), hline.after=c(-1, 0, 5, 6, 11), 
          only.contents = TRUE)
```
\end{tabular}
\end{minipage}
\end{table}

```{r, tree.choixHyperparametre, result=FALSE}
choixCP <- tree.gridResult.gini$cp[which.max(tree.gridResult.gini$ROC)]
choixMinbucket <- 
    tree.gridResult.gini$minbucket[which.max(tree.gridResult.gini$ROC)]
```
Les résultats, peut importe la fonction de perte utilisé, sont similaire dans
pour l'hyperparamètres _minbucket_. Par contre, le modèle utilisant l'indice de
_Gini_ est optimal pour une valeur de _cp_ plus élevé, résultant en un arbre
moins complexe. Il est donc meileur de prendre ce modele puisqu'il risque
potentiellement de moins sur-apprendre les données d'apprentissage. Ainsi, le
modèle choisis utilisira un _cp_ de `r format(choixCP, digits=5, scientific =
FALSE)`, un _minbucket_ de `r choixMinbucket` et utilisera l'indice de
_Gini_ comme fonction de perte.

```{r, results='asis', fig.height=4, fig.width=7}
rpart.plot(tree.final, type=1, fallen.leaves = FALSE, tweak=1.1)
```


\newpage

## Un ensemble d’arbres de décision agrégées par bagging

```{r, loading_bagging}
require(randomForest)
require(rpart)
require(rpart.plot)
require(caret)
require(pROC)

mod_base_bag <- readRDS("../src/05-Bagging/mod_bag_base.rds")
result_opt_bag <- readRDS("../src/05-Bagging/result_opt_bag_save.rds")
result_opt_bag <- as.data.frame(result_opt_bag)
colnames(result_opt_bag) <- c("nodesize", "AUC")

mod_base_forest <- readRDS("../src/06-RandomForest/mod_forest_base.rds")
result_opt_forest <- readRDS("../src/06-RandomForest/result_opt_rf_save.rds")
result_opt_forest <- as.data.frame(result_opt_forest)
colnames(result_opt_forest) <- c("nodesize", "mtry", "AUC")
```

Le modèle d'ensemble d’arbres de décision agrégées par bagging permet de d'obtenir
des prévisions à partir d'un nombre d'arbre de décision, chacun ajuster par un 
échantillon bootstrap des l'échantillon d'entrainement. Une prévision du bagging
correspondera à la classe prédite majoritaire (renouvellement ou non renouvellement)
par les arbres declassifications puisqu'il s'agit d'un problème de classification.
Chacun des arbres utilise l'indice de Gini comme fonction de perte. Nous avons fait
ce choix puisque qu'à la section précédente portant sur l'arbre de 
classification, l'indice de Gini a été préféré à l'entropie croisée.


Avant l'optimisation d'hyperparamètre il est nécessaire de déterminer un nombre
d'arbre suffisant pour améliorer les prévisions. Pour ce faire, nous ajustons
un modèle bagging avec un nombre d'arbre plus élevé que nécessaire avec
l'échantillon d'entrainement. Ce modèle bagging de base n'a pas de pas de
contrainte appliquée sur chacun des arbres puisque ce sera optimiser par
la suite. Pour déterminer le nombre d'arbre adéquat nous avons tracé un graphique
du taux d'erreur OOB en fonction du nombre d'arbres.

```{r, plot_OOB_bagging, results='asis', fig.height=4, fig.cap="Taux d'erreur OOB en fonction du nombre d'arbre B"}
plot(1:dim(mod_base_bag$err.rate)[1], mod_base_bag$err.rate[,1],
     type="l", xlab="B", ylab="Taux d'erreur OOB")
```

Comme on peut le voir dans le graphique, le taux d'erreur OOB se stabilise à partir
d'un nombre d'arbre de 100 et plus. Le nombre d'abre de notre bagging sera donc de 100.

\newpage

L'hyperparamètre qui sera optimisé par la suite pour augmenter les performances
du bagging est le paramètre de la taille minimale d'un noeud (nodesize) pour chacun
des arbres du modèle bagging. Des tailles de noeuds mininales partant de 2 allant
jusqu'à 40 seront testées. Pour l'optimisation, on s'intéressera à maximiser
la métrique AUC. Pour chaque taille de noeud, un AUC moyen sera calculée à partir
d'une validation croisée par 4 ensembles. Voici les résultats obtenus de la
validation croisée.


\begin{table}[!ht]
\begin{minipage}{0.23\linewidth}
\centering
\begin{tabular}{cc}
\hline
\multicolumn{2}{c}{} \\
```{r, results='asis'}
Rmd_table(result_opt_bag[1:10, ], caption="", label="",
          digits=c(0,0,4), hline.after=c(-1, 0, 10), 
          only.contents = TRUE)
```
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.23\linewidth}
\centering
\begin{tabular}{cc}
\hline
\multicolumn{2}{c}{} \\
```{r, results='asis'}
Rmd_table(result_opt_bag[11:20, ], caption="", label="",
          digits=c(0,0,4), hline.after=c(-1, 0, 10), 
          only.contents = TRUE)
```
\end{tabular}
\end{minipage}
\begin{minipage}{0.23\linewidth}
\centering
\begin{tabular}{cc}
\hline
\multicolumn{2}{c}{} \\
```{r, results='asis'}
Rmd_table(result_opt_bag[21:30, ], caption="", label="",
          digits=c(0,0,4), hline.after=c(-1, 0, 10), 
          only.contents = TRUE)
```
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.23\linewidth}
\centering
\begin{tabular}{cc}
\hline
\multicolumn{2}{c}{} \\
```{r, results='asis'}
Rmd_table(result_opt_bag[31:39, ], caption="", label="",
          digits=c(0,0,4), hline.after=c(-1, 0, 2, 3, 9), 
          only.contents = TRUE)
```
\end{tabular}
\end{minipage}
\end{table}


On constate que l'AUC est maximisé lorsque nodesize prend une valeure 34. Le modèle d'ensemble d’arbres de décision agrégées par bagging choisi aura donc un nombre d'abres de 100 et une taille minimale d'un noeud pour les arbres de 34.  

\newpage

## Forêt aléatoire

La forêt aléatoire permet elle aussi d'obtenir des prévisions à partir de prévisions aggrégées d'arbre de classification. La fonction de perte utilisée à chaque arbre est encore l'indice de Gini pour la même raison mentionnée dans la section du Bagging. Cependant, un changement par rapport au bagging est la taille de l'échantillon boostrap pour ajuster les arbres. Puisque notre jeu de données est assez volumineux et que cette fois-ci, nous devrons optimiser deux hyperparamètres plutôt qu'un, nous avons choisi de prendre une taille d'échantillon bootsrap correspondant à 50\% de l'échantillon d'entraînement pour réduire les temps de calcul de l'optimisation.

Avant de procéder à l'optimisation il est encore nécessaire de déterminer un nombre d'arbre sufffisant pour la forêt. Pour ce faire, nous avons ajuster un modèle de forêt basique avec un nombre d'arbre plus élevé que nécessaire. Comme pour le bagging, pour déterminer le nombre d'arbre adéquatnous avons tracé un graphique du taux d'erreur OOB en fonction du nombre d'arbres.

```{r, plot_OOB_forest, results='asis', fig.height=4, fig.cap="Taux d'erreur OOB en fonction du nombre d'arbre B"}
plot(1:dim(mod_base_forest$err.rate)[1], mod_base_forest$err.rate[,1],
     type="l", xlab="B", ylab="Taux d'erreur OOB")
```

\newpage

Comme on peut le voir dans le graphique, le taux d'erreur OOB se stabilise à partir
d'un nombre d'arbre de 100 et plus comme le bagging. Le nombre d'arbres de notre forêt aléatoire sera donc de 100. Il y a deux hyperparamètres qui seront optimisé soient la taille minimale d'un noeud (nodesize) et le nombre de prédicteurs choisi de façon aléatoire (mtry) à chaque embranchement des arbres de la forêt aléatoire. Pour chaque nodesize allant de 5 jusqu'à 40 par multiple de 5, un mtry optimal est obtenu par validation croisée par 5 ensembles. Le métrique d'intérêt de l'optimisatiojn est l'AUC. Ce sont donc, des couples optimaux qui sont obtenu. Voici les résultats obtenus.

\begin{table}[!ht]
\centering
\begin{tabular}{ccc}
\hline
\multicolumn{3}{c}{} \\
```{r, results='asis'}
Rmd_table(result_opt_forest, caption="", label="",
          digits=c(0,0,0,4), hline.after=c(-1, 0, 3, 4, 8), 
          only.contents = TRUE)
```
\end{tabular}
\end{table}



On constate que le couples d'hyperparamètres qui a obtenu l'AUC le plus élevé correspond à un nodesize de 20 et un mtry de 10. En somme, la forest aléatoire choisi aura donc les caractéristiques suivantes: 

\begin{itemize}
\item La taille d'échantillon boostrap est de 50\% de l'échantillon d'entrainement
\item La forest aléatoire composée de 100 arbres
\item La taille minimale d'un noeud (nodesize) d'un arbre est de 20 observations
\item Le nombre de prédicteurs choisis de façon aléatoire (mtry) à chaque embranchement est de 10
\end{itemize}

\newpage

# Bibliographie
